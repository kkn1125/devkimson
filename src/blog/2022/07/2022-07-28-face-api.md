---
slug: "/face-api01"
layout: post
date: 2022-07-28 13:41:54 +0900
title: "[JAVASCRIPT] face-api.js 기본 사용법을 알아보자"
author: Kimson
categories: [library]
image: /images/post/covers/TIL-javascript.png
tags: [ai, face detect, til]
description: "face-api.js

이번에 토이 프로젝트를 작게 실행하고 있습니다. 토이 프로젝트를 아직 개발 단계이자 완전 초기 단계라 어떻다 설명 할 내용이 없지만 핵심적으로 사용되는 기술은 안면 인식과 텍스트를 분석해서 감정을 추출하는 것이 이번 프로젝트의 핵심 기술이라 할 수 있습니다.

이번 포스팅은 프로젝트에 대한 내용이 아니므로 다음에 완성되고 나면 소개를 드릴까 생각 중 입니다.

먼저 사용했던 face-api.js를 다루는 데 어려움을 겪었던 내용을 반영하여 저와 같은 어려움에 처한 분에게 조금이나마 도움이 되고자 기록을 남깁니다.

face-api는 tensorflow.js를 기반으로 개발되었으며 canvas에 안면 인식 프레임과 인식 개체명, 감정분석 결과를 표시하는 방식입니다.

대게 node환경에서 작성된 예제가 많이 있습니다. 그래서 react + typescript 기반의 프로젝트에서 사용하기 위해 참고했던 여러 블로그나 사이트 등의 링크를 포스팅 하단에 첨부하였으니 필요하시다면 참고바랍니다."
featured: true
hidden: false
rating: 4.5
toc: true
profile: false
istop: true
keysum: false
keywords: ""
published: true
---

# face-api.js

이번에 토이 프로젝트를 작게 실행하고 있습니다. 토이 프로젝트를 아직 개발 단계이자 완전 초기 단계라 어떻다 설명 할 내용이 없지만 핵심적으로 사용되는 기술은 안면 인식과 텍스트를 분석해서 감정을 추출하는 것이 이번 프로젝트의 핵심 기술이라 할 수 있습니다.

이번 포스팅은 프로젝트에 대한 내용이 아니므로 다음에 완성되고 나면 소개를 드릴까 생각 중 입니다.

먼저 사용했던 `face-api.js`를 다루는 데 어려움을 겪었던 내용을 반영하여 저와 같은 어려움에 처한 분에게 조금이나마 도움이 되고자 기록을 남깁니다.

`face-api`는 `tensorflow.js`를 기반으로 개발되었으며 `canvas`에 안면 인식 프레임과 인식 개체명, 감정분석 결과를 표시하는 방식입니다.

대게 `node`환경에서 작성된 예제가 많이 있습니다. 그래서 `react` + `typescript` 기반의 프로젝트에서 사용하기 위해 참고했던 여러 블로그나 사이트 등의 링크를 포스팅 하단에 첨부하였으니 필요하시다면 참고바랍니다.

## face-api의 주요 기능

주요 기능은 이미지나 영상에서 인물의 안면 인식과 인식된 인물 정보 커스터마이징, 인지된 인물의 안면에 대한 감정 분석, 나이와 성별 판별이 있습니다.

그 중에서도 이미지에서의 감지와 영상에서의 감지 방법과 `react`에서 사용하는 방법을 기록하려 합니다. 때문에 다소 포스팅이 길어질 수 있으므로 포스팅을 나누는 계획은 다음으로 하고, 지금은 한 번에 모두 정리하겠습니다. 내용이 길더라도 양해바랍니다. `node`에서 사용하는 방법은 따로 다루지 않고 제가 참고 했던 `node`환경의 예제를 링크로 남겨두겠습니다.

## 각 환경에서 공통으로 필요한 소스파일 및 설정

`face-api`를 사용하는데 있어 모델 파일이 필요합니다. 없으면 작동을 안하니 꼭 파일을 받아 사용할 모델파일을 적용(load)해야하니 주의 하시길 바랍니다. model load 하지 않고 작업해서 영영 작동 안되는 줄 알았습니다...

<figure class="text-center">
<span class="w-inline-block">
   <img src="https://user-images.githubusercontent.com/71887242/181424910-0fab93f5-5d50-43f1-b412-3c822481c387.png" alt="sample" title="sample">
   <figcaption>모델 소스 파일들</figcaption>
</span>
</figure>

모델 파일은 [justadudewhohacks](https://github.com/justadudewhohacks/face-api.js)의 `weight` 디렉토리에 `shard`라는 파일과 `*manifest.json` 파일이 있는 것을 볼 수 있습니다. 저장소를 다운받아 이 모델 파일들을 꼭 받아야 합니다. 이 모델 파일들이 없으면 `face-api`는 기본적으로 아예 사용 못 합니다.

그리고 css를 조금 설정해두어야 합니다. 저는 글로벌 스타일로 설정하였습니다.

```css
div#wrap {
  position: relative;
}

div#wrap canvas {
  position: absolute;
  top: 0;
  left: 0;
}
```

`face-api`는 앞서 말 했듯이 `canvas`로 안면인식 정보를 표시합니다. 영상 또는 이미지에 오버되어서 표시되야 하기 때문에 위와 같은 `css` 설정이 필요합니다.

## react + typescript 환경에서 face-api 적용하기

`react`에서 적용할 때 주의점은 크게 없습니다. `react`의 `useEffect`를 이해한다면 무리없이 적용해서 자유롭게 사용할 수 있다고 생각합니다.

`node`와 `react`에 적용하는 방법은 크게 보면 아래와 같은 순서로 공통 진행됩니다.

1. `model load`
2. `detection` 설정

`detection` 설정 시 세부적으로는 아래와 같은 내용이 있습니다.

1. `image` or `media` 사용 (`input`으로 이미지 로드 혹은 navigator api로 media 요청)
2. `face-api`를 사용하여 `detection` 설정
3. `canvas`를 생성하여 인식된 안면에 정보 표시

### image기반으로 안면 인식할 때

#### model load

모델을 불러오는 것은 간단합니다. 모델 소스파일은 아래 이미지의 파일구조를 참고하여 받아오시면 됩니다.

<figure class="text-center">
<span class="w-inline-block">
   <img src="https://user-images.githubusercontent.com/71887242/181429611-819ccea1-18e0-4a75-bc25-002da7a1bb6a.png" alt="sample" title="sample">
   <figcaption>파일구조 참고</figcaption>
</span>
</figure>

모델을 받아오고 detection을 설정하는 예제를 보겠습니다.

```tsx
import React from "react";
import * as faceapi from "face-api.js";

const MODEL_URL = process.env.PUBLIC_URL + "/models";

function App() {
  const wrapRef = useRef<HTMLDivElement>(null);

  const [isStartDetect, setIsStartDetect] = useState<boolean>(false);
  const [modelsLoaded, setModelsLoaded] = useState<boolean>(false);
  const [peopleAmount, setPeopleAmount] = useState(0);

  useEffect(() => {
    Promise.all([
      faceapi.nets.faceRecognitionNet.loadFromUri(MODEL_URL),
      faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL),
      faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL),
      faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL),
    ]).then(() => {
      setModelsLoaded(true);
    });
  }, []);

  const handleLoadFaceImage = () => {};

  return (
    <div>
      <h2>Face-Api Test</h2>
      <ul>
        <li>model loaded: {modelsLoaded.toString()}</li>
        {modelsLoaded && isStartDetect && (
          <li>감지된 인원: {peopleAmount} 명</li>
        )}
      </ul>

      <div
        ref={wrapRef}
        style={{
          position: "relative",
        }}
        hidden={!isStartDetect}
      />

      {modelsLoaded ? (
        <input type='file' onChange={handleLoadFaceImage} />
      ) : (
        <div>모델을 불러오는 중 입니다...</div>
      )}
    </div>
  );
}

export default App;
```

`Promise`로 필요한 모델파일을 `load`해야합니다. 보기 쉽도록 몇가지 `state`를 사용했습니다. model이 다 로드되면 input을 표시하여 이미지를 가져올 준비를 합니다.

그 다음 `handleLoadFaceImage` 함수를 완성해봅시다.

#### detection 설정

```tsx
import React from "react";
import * as faceapi from "face-api.js";

const MODEL_URL = process.env.PUBLIC_URL + "/models";

function App() {
  const wrapRef = useRef<HTMLDivElement>(null);

  const [isStartDetect, setIsStartDetect] = useState<boolean>(false);
  const [modelsLoaded, setModelsLoaded] = useState<boolean>(false);
  const [peopleAmount, setPeopleAmount] = useState(0);

  useEffect(() => {
    // promise ...
  }, []);

  const handleLoadFaceImage = () => {
    const file = e.currentTarget.files[0];
    // file 정보를 img 요소로 변환
    const img = await faceapi.bufferToImage(file);
    // 이미지 정보를 기반으로 canvas 요소 생성
    const canvas = faceapi.createCanvasFromMedia(img);

    if (wrapRef.current) {
      wrapRef.current.append(img);
      wrapRef.current.append(canvas);
    }

    // 이미지 사이즈를 canvas에 맞추기 위한 설정
    const displaySize = {
      width: img.width,
      height: img.height,
    };

    // canvas 사이즈를 맞춤
    faceapi.matchDimensions(canvas, displaySize);

    // 안면 인식이 시작되는 부분이며, 옵션을 설정하는 부분
    const detections = await faceapi
      .detectAllFaces(img, new TinyFaceDetectorOptions())
      .withFaceLandmarks() // 안면 인식 프레임
      .withFaceExpressions() // 감정 분석
      .withFaceDescriptors(); // 인식된 얼굴에 주석을 설정

    // 인식 여부를 true 로 설정
    setIsStartDetect(true);
    // 인식 된 개체 수 설정
    setPeopleAmount(detections.length);

    const resizedDetections = faceapi.resizeResults(detections, displaySize);

    resizedDetections.forEach((detection) => {
      const box = detection.detection.box;
      const drawBox = new faceapi.draw.DrawBox(box, { label: "Face" });
      drawBox.draw(canvas);
    });

    // 인식 프레임과 겹치므로 제외
    // faceapi.draw.drawDetections(canvas, resizedDetections);

    // 랜드마크 렌더링 및 감정 데이터 표시
    faceapi.draw.drawFaceLandmarks(canvas, resizedDetections);
    faceapi.draw.drawFaceExpressions(canvas, resizedDetections);
  };

  return <div>...</div>;
}

export default App;
```

대부분 코드에 대한 설명은 주석으로 달아두었습니다. 서두에 알려드렸듯이 모델이 로드되고, 이미지와 캔버스를 생성하고, `detection`을 설정하여 정보를 출력하기 위해 `draw`메서드를 사용해서 `canvas`에 데이터를 그립니다.

실행과정을 미리보기 식으로 보여드리면 아래와 같습니다.

<figure class="text-center">
<span class="w-inline-block">
   <img src="https://user-images.githubusercontent.com/71887242/181443770-c4e6279d-4a80-4245-8519-a7252f674d59.png" alt="sample" title="sample">
   <figcaption>모델 로드 전</figcaption>
</span>
</figure>

<figure class="text-center">
<span class="w-inline-block">
   <img src="https://user-images.githubusercontent.com/71887242/181443987-5031707f-8238-479b-b5fe-88a76f223d31.png" alt="sample" title="sample">
   <figcaption>모델 로드 후</figcaption>
</span>
</figure>

<figure class="text-center">
<span class="w-inline-block">
   <img src="https://user-images.githubusercontent.com/71887242/181444121-dc5d02d1-298e-4913-94b0-e33168a06e27.png" alt="sample" title="sample">
   <figcaption>이미지 가져오기</figcaption>
</span>
</figure>

<figure class="text-center">
<span class="w-inline-block">
   <img src="https://user-images.githubusercontent.com/71887242/181444213-d52ec982-c562-4364-8305-6266f821a1f9.png" alt="sample" title="sample">
   <figcaption>인식 결과 표시</figcaption>
</span>
</figure>

그 결과는 아래와 같습니다. 사진은 연예인 중에 가장 좋아하고 존경하는 분인 유재석님 입니다. happy가 백분율로 1이라니 밝게 웃는 모습이 잘 인식되어서 좋네요 😁

만일 input 으로 로드하는게 아닌 로컬의 파일을 불러와 로드시켜야할 떄가 있습니다. 예를 들면 서버를 통해 데이터베이스에 이미지 경로를 가져와 서버에 업로드된 이미지 파일을 자동으로 불러와야한다면 아래와 같이 변경해서 사용할 수도 있습니다.

이미지의 파일 위치는 아래와 같습니다.

<figure class="text-center">
<span class="w-inline-block">
   <img src="https://user-images.githubusercontent.com/71887242/181453886-43390d2e-1825-49fe-9668-2e5aa577ad97.png" alt="sample" title="sample">
   <figcaption>이미지 파일 구조</figcaption>
</span>
</figure>

```tsx
import React from "react";
import * as faceapi from "face-api.js";

const MODEL_URL = process.env.PUBLIC_URL + "/models";

function App() {
  // ref, states ...

  Promise.all([
    faceapi.nets.faceRecognitionNet.loadFromUri(MODEL_URL),
    faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL),
    faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL),
    faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL),
  ]).then(() => {
    setModelsLoaded(true);
    handleLoadFaceImage(); // 추가
  });

  const handleLoadFaceImage = () => {
    /* const file = e.currentTarget.files[0];
    const img = await faceapi.bufferToImage(file); */

    // 로컬 이미지를 경로를 통해 가져오기
    const img = await faceapi.fetchImage(require("./imgs/you.jpg"));
    const canvas = faceapi.createCanvasFromMedia(img);

    if (wrapRef.current) {
      wrapRef.current.append(img);
      wrapRef.current.append(canvas);
    }

    // ... 이하 동일
  };

  return (
    <div>
      <h2>Face-Api Test</h2>
      <ul>
        <li>model loaded: {modelsLoaded.toString()}</li>
        {modelsLoaded && isStartDetect && (
          <li>감지된 인원: {peopleAmount} 명</li>
        )}
      </ul>

      <div
        ref={wrapRef}
        style={{
          position: "relative",
        }}
        hidden={!isStartDetect}
      />

			<!-- 변경 부분 -->
      {modelsLoaded && !isStartDetect && <div>이미지 인식 중...</div>}
      {!modelsLoaded && <div>모델을 불러오는 중 입니다...</div>}
    </div>
  );
}
```

생각보다 건드릴 부분은 많이 없습니다. 이전 코드에서 변수명을 유지한 채 변경된 부분에 주석을 달아두었으니 보기 어렵지 않으시리라 생각됩니다.

먼저 `bufferToImage`로 받아오던 구문을 `fetchImage`로 경로를 사용해 이미지 요소를 만듭니다.

그리고 인식 할 이미지를 가져오는 방식이 바뀌었으니 `html`부분도 변경하였습니다. 추가로 이미지를 분석하는데 걸리는 시간동안에는 "이미지 인식 중" 이라는 문구를 추가한 것이 전부 입니다.

### video기반으로 안면 인식할 때

`video`기반으로 할 때는 조금 달라집니다. 달라지는 부분을 짚어보면서 정리하겠습니다. 테스트에 앞서 필요한 것은 현재 테스트할 `pc` 혹은 `labtop`에 화상카메라가 탑재되어 있어야합니다. 없으면 못 합니다 😥

아예 못하지는 않습니다. 방법은 포스팅 제일 하단에 정리해두었으니 참고 바랍니다 😎

#### model load

```tsx
import React, { useEffect, useRef, useState } from "react";
import { TinyFaceDetectorOptions } from "face-api.js";
import * as faceapi from "face-api.js";

const MODEL_URL = process.env.PUBLIC_URL + "/models";

// 비디오 사이즈 설정
const constraints = {
  video: {
    width: 640,
    height: 480,
  },
  audio: false,
};

function App() {
  const wrapRef = useRef<HTMLDivElement>(null);
  const videoRef = useRef<HTMLVideoElement>(null);

  const [isStartDetect, setIsStartDetect] = useState<boolean>(false);
  const [modelsLoaded, setModelsLoaded] = useState<boolean>(false);

  const startDetecting = async () => {
    // model load
    const loadModels = async () => {
      const MODEL_URL = process.env.PUBLIC_URL + "/models";

      Promise.all([
        faceapi.nets.faceRecognitionNet.loadFromUri(MODEL_URL),
        faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL),
        faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL),
        faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL),
        // video 에서 로드된 이미지 매칭 시 아래 모델이 필요 함.
        faceapi.nets.ssdMobilenetv1.loadFromUri(MODEL_URL),
        faceapi.nets.ageGenderNet.loadFromUri(MODEL_URL),
      ]).then(() => {
        setModelsLoaded(true);
        startVideo();
      });
    };

    loadModels();
  };

  // 영상 권한 요청
  const startVideo = () => {
    setIsStartDetect(true);

    navigator.mediaDevices
      .getUserMedia(constraints)
      .then((stream) => ((videoRef.current as any).srcObject = stream))
      .catch((err) => console.error(err));
  };

  return (
    <div>
      <h2>Face-Api Video Test</h2>
      <ul>
        <li>model loaded: {modelsLoaded.toString()}</li>
      </ul>

      <div ref={wrapRef}>
        <video
          ref={videoRef}
          autoPlay
          muted
          onPlay={onPlay}
          width={640}
          height={480}
        />
      </div>

      <button onClick={startDetecting}>영상 권한 호출</button>
    </div>
  );
}

export default App;
```

이미지 로드하는 방식과 다른 점은 영상을 호출하는 로직이 필요한 것과 `video`가 `play`될 때 발생하는 이벤트를 이용해서 `detection` 설정 및 `setInterval` 등의 루프 함수를 작성하여 영상에 맞추어 인식 박스를 다시 그려주는 것 입니다.

다음으로 `detection`을 설정하겠습니다.

#### detection 설정

```tsx
import React, { useEffect, useRef, useState } from "react";
import { TinyFaceDetectorOptions } from "face-api.js";
import * as faceapi from "face-api.js";

const MODEL_URL = process.env.PUBLIC_URL + "/models";

// 비디오 사이즈 설정
const constraints = {
  video: {
    width: 640,
    height: 480,
  },
  audio: false,
};

function App() {
  // refs, states ...

  // 라벨링 할 인물 이미지 로컬에서 가져오기
  const loadImage = async () => {
    // 업로드 된 이미지 이름을 배열에 담아 라벨링 합니다.
    const labels = ["you", "kimson"];

    return Promise.all(
      labels.map(async (label) => {
        const images = await faceapi.fetchImage(require(`./imgs/${label}.jpg`));
        const descriptions = [];
        const detections = await faceapi
          .detectSingleFace(images)
          .withFaceLandmarks()
          .withFaceDescriptor();
        descriptions.push(detections.descriptor);

        return new faceapi.LabeledFaceDescriptors(label, descriptions);
      })
    );
  };

  const onPlay = async () => {
    // 이미지 정보를 기반으로 canvas 요소 생성
    const canvas = faceapi.createCanvasFromMedia(videoRef.current);
    wrapRef.current.append(canvas);

    // 영상 사이즈를 canvas에 맞추기 위한 설정
    const displaySize = {
      width: videoRef.current.width,
      height: videoRef.current.height,
    };

    // canvas 사이즈를 맞춤
    faceapi.matchDimensions(canvas, displaySize);

    // 로컬 대조 이미지 가져오기
    const labeledFaceDescriptors = await loadImage();

    // 안면 인식 부분
    const faceDetecting = async () => {
      const detections = await faceapi
        .detectAllFaces(videoRef.current, new faceapi.TinyFaceDetectorOptions())
        .withFaceLandmarks()
        .withFaceExpressions()
        .withAgeAndGender()
        .withFaceDescriptors();

      const resizedDetections = faceapi.resizeResults(detections, displaySize);

      // canvas 초기화
      canvas.getContext("2d").clearRect(0, 0, canvas.width, canvas.height);

      const faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors, 0.6);

      resizedDetections.forEach((detection, i) => {
        const matched = resizedDetections[i];
        const box = matched.detection.box;
        const label = faceMatcher.findBestMatch(matched.descriptor).toString();
        const drawBox = new faceapi.draw.DrawBox(box, {
          label: label,
        });
        drawBox.draw(canvas);
        // 기본 안면 인식 테두리, 겹치므로 제외
        // faceapi.draw.drawDetections(canvas, resizedDetections);
        // 감정 읽기
        faceapi.draw.drawFaceExpressions(canvas, resizedDetections);
      });
    };

    const loop = () => {
      faceDetecting();
      setTimeout(loop, 1);
    };
    setTimeout(loop, 1);
  };

  const startDetecting = async () => {
    const loadModels = async () => {
      // promise ...
    };

    loadModels();
  };

  const startVideo = () => {
    // getMedia...
  };

  return <div>...</div>;
}

export default App;
```

이미지 인식에서 빠진 부분이 있습니다. 라벨링된 인물을 대조하는 부분인데요. 비디오에서 하는 방법과 동일하기 때문에 생략하였습니다. 하나씩 모두 다루기에는 포스팅이 너무나 길어질 것 같아서 입니다.

비디오에서 인식할 때 라벨링된 인물이 감지되면 detections의 배열에 담기기 때문에 forEach를 돌아 매칭된 인물만을 조회하기 위해 forEach를 사용했습니다.

실행과정을 미리보기 식으로 보여드리면 아래와 같습니다.

<figure class="text-center">
<span class="w-inline-block">
   <img src="https://user-images.githubusercontent.com/71887242/181463575-97e540fd-35c1-4e1a-82a3-cd7f03f6bcb4.png" alt="sample" title="sample">
   <figcaption>모델 로드 전</figcaption>
</span>
</figure>

<figure class="text-center">
<span class="w-inline-block">
   <img src="https://user-images.githubusercontent.com/71887242/181464330-5993dfbd-81f5-4cae-b020-226522680136.png" alt="sample" title="sample">
   <figcaption>모델 + 영상 표시 + 실시간 분석</figcaption>
</span>
</figure>

`react`에서 `face-api.js`를 적용하는 예시를 정리 해 보았습니다. 영상 테스트는 저를 사용할 수 밖에 없어서 양해바랍니다 😅

## 마무리

기존에 node환경에 있던 예시 코드를 하나씩 분석하면서 react로 마이그레이션하는 작업을 하다보니 똑같이 node환경에 따라서 코드를 치는 것보다 코드 구조나 원리가 더 와닿는 것 같습니다.

face-api.js에서 설명하는 함수를 정독해보면서 오류가 발생하더라도 대충이나마 어디서 뭐가 문제인지 감이와서 별 무리 없이 적용에 성공한 것 같습니다.

인식률이 높은지는 잘 모르겠습니다만 분명히 이 기술을 이용하면 다양한 아이디어를 내서 웹서비스에 적용할 수 있을 것 같습니다. 지금 당장에 떠오르는 것은 화상 채팅룸에 지정된 인원이 참가하면 자동으로 출석체크 된다던지 등의 기능을 한 번 테스트 해보고 싶네요 🤔

여기까지 읽어주신 분께 감사드립니다 😊

> 인식할 대상의 이미지를 다양하게 준비해두면 더 높은 확률로 인식할 수 있을 것 같습니다만 react환경에서 사용하는 예시를 들었기 때문에 간단하게만 작성했습니다.

---

📚 함께 보면 좋은 내용

[justadudewhohacks::Github Repository](https://github.com/justadudewhohacks/face-api.js)

[WebDevSimplified::영상 강의](https://www.youtube.com/watch?v=CVClHLwv-4I&ab_channel=WebDevSimplified)

[Muftaudeen Jimoh medium::Simple AI Face and emotion Recognition With React](https://blog.devgenius.io/simple-ai-face-and-emotion-recognition-with-react-da2921e6075e)

[Muftaudeen Jimoh medium::Simple AI Face and emotion Recognition With React](https://blog.devgenius.io/simple-ai-face-and-emotion-recognition-with-react-da2921e6075e)
